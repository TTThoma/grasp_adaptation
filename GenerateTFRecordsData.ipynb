{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dof6/.local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "float64\n",
      "(3,)\n",
      "float64\n",
      "(4,)\n",
      "float64\n",
      "(50, 50, 3)\n",
      "float32\n",
      "(50, 50, 1)\n",
      "float32\n",
      "()\n",
      "float64\n",
      "()\n",
      "float64\n",
      "21403\n",
      "95863\n",
      "5377\n",
      "24137\n"
     ]
    }
   ],
   "source": [
    "training_writer = tf.python_io.TFRecordWriter('./grasp_data/t_shape/training.tfrecord')\n",
    "test_writer = tf.python_io.TFRecordWriter('./grasp_data/t_shape/test.tfrecord')\n",
    "            \n",
    "ratio = 0.8\n",
    "training_positive = 0\n",
    "training_negative = 0\n",
    "test_positive = 0\n",
    "test_negative = 0\n",
    "for file_name in os.listdir('./grasp_data/t_shape/'):\n",
    "    if \".npy\" in file_name:\n",
    "        data = np.load('./grasp_data/t_shape/'+file_name,allow_pickle=True)[()]\n",
    "        \n",
    "        grasp_quality_list = np.asarray(data['grasp_quality_list']).squeeze()\n",
    "        eef_position_list = np.asarray(data['eef_position_list']).squeeze()\n",
    "        eef_quaternion_list = np.asarray(data['eef_quaternion_list']).squeeze()\n",
    "        eef_euler_list = np.asarray(data['eef_euler_list']).squeeze()\n",
    "        cropped_color_image_list = np.asarray(data['cropped_color_image_list']).squeeze()\n",
    "        cropped_depth_image_list = np.asarray(data['cropped_depth_image_list']).squeeze()\n",
    "        grasp_width_list = np.asarray(data['grasp_width_list']).squeeze()\n",
    "        grasp_depth_list = np.asarray(data['grasp_depth_list']).squeeze()\n",
    "\n",
    "        for grasp_quality, eef_position, eef_quaternion, eef_euler, cropped_color_image, cropped_depth_image, grasp_width, grasp_depth in zip(grasp_quality_list, eef_position_list, eef_quaternion_list, eef_euler_list, cropped_color_image_list, cropped_depth_image_list, grasp_width_list, grasp_depth_list):\n",
    "            cropped_depth_image = cropped_depth_image[:,:,np.newaxis]\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'eef_position': tf.train.Feature(bytes_list=tf.train.BytesList(value=[eef_position.astype(np.float32).tostring()])),\n",
    "                'eef_quaternion': tf.train.Feature(bytes_list=tf.train.BytesList(value=[eef_quaternion.astype(np.float32).tostring()])),\n",
    "                'eef_euler': tf.train.Feature(bytes_list=tf.train.BytesList(value=[eef_euler.astype(np.float32).tostring()])),\n",
    "                'cropped_color_image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[cropped_color_image.astype(np.float32).tostring()])),\n",
    "                'cropped_depth_image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[cropped_depth_image.astype(np.float32).tostring()])),\n",
    "                'grasp_width': tf.train.Feature(float_list=tf.train.FloatList(value=[grasp_width.astype(np.float32)])),\n",
    "                'grasp_depth': tf.train.Feature(float_list=tf.train.FloatList(value=[grasp_depth.astype(np.float32)])),\n",
    "                'grasp_quality': tf.train.Feature(float_list=tf.train.FloatList(value=[grasp_quality.astype(np.float32)]))\n",
    "            }))\n",
    "            if np.random.uniform() < ratio:\n",
    "                training_writer.write(example.SerializeToString())\n",
    "                if grasp_quality == 0.0:\n",
    "                    training_negative+=1\n",
    "                else:\n",
    "                    training_positive+=1\n",
    "            else:\n",
    "                test_writer.write(example.SerializeToString())\n",
    "                if grasp_quality == 0.0:\n",
    "                    test_negative+=1\n",
    "                else:\n",
    "                    test_positive+=1\n",
    "                \n",
    "print(grasp_quality.shape)\n",
    "print(grasp_quality.dtype)\n",
    "print(eef_position.shape)\n",
    "print(eef_position.dtype)\n",
    "print(eef_quaternion.shape)\n",
    "print(eef_quaternion.dtype)\n",
    "print(cropped_color_image.shape)\n",
    "print(cropped_color_image.dtype)\n",
    "print(cropped_depth_image.shape)\n",
    "print(cropped_depth_image.dtype)\n",
    "print(grasp_width.shape)\n",
    "print(grasp_width.dtype)\n",
    "print(grasp_depth.shape)\n",
    "print(grasp_depth.dtype)\n",
    "\n",
    "print(training_positive)\n",
    "print(training_negative)\n",
    "print(test_positive)\n",
    "print(test_negative)\n",
    "\n",
    "training_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    features = {\n",
    "        'eef_position': tf.FixedLenFeature([], tf.string),\n",
    "        'eef_quaternion': tf.FixedLenFeature([], tf.string),\n",
    "        'eef_euler': tf.FixedLenFeature([], tf.string),\n",
    "        'cropped_color_image': tf.FixedLenFeature([], tf.string),\n",
    "        'cropped_depth_image': tf.FixedLenFeature([], tf.string),\n",
    "        'grasp_width': tf.FixedLenFeature([], tf.float32),\n",
    "        'grasp_depth': tf.FixedLenFeature([], tf.float32),\n",
    "        'grasp_quality': tf.FixedLenFeature([], tf.float32)\n",
    "    }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    gripper_position = tf.decode_raw((parsed_features['eef_position']), tf.float32)\n",
    "    gripper_position = tf.reshape(gripper_position,[3])\n",
    "    \n",
    "    gripper_quaternion = tf.decode_raw((parsed_features['eef_quaternion']), tf.float32)\n",
    "    gripper_quaternion = tf.reshape(gripper_quaternion,[4])\n",
    "    \n",
    "    gripper_euler = tf.decode_raw((parsed_features['eef_euler']), tf.float32)\n",
    "    gripper_euler = tf.reshape(gripper_euler,[3])\n",
    "    \n",
    "    cropped_color_image = tf.decode_raw((parsed_features['cropped_color_image']), tf.float32)\n",
    "    cropped_color_image = tf.reshape(cropped_color_image,[50, 50, 3])\n",
    "    \n",
    "    cropped_depth_image = tf.decode_raw((parsed_features['cropped_depth_image']), tf.float32)\n",
    "    cropped_depth_image = tf.reshape(cropped_depth_image,[50, 50, 1])\n",
    "    \n",
    "    grasp_depth = parsed_features['grasp_depth']\n",
    "    grasp_depth = tf.reshape(grasp_depth,[1])\n",
    "    \n",
    "    grasp_width = parsed_features['grasp_width']\n",
    "    grasp_width = tf.reshape(grasp_width,[1])\n",
    "    \n",
    "    grasp_quality = parsed_features['grasp_quality']\n",
    "    grasp_quality = tf.reshape(grasp_quality,[1])\n",
    "\n",
    "    return gripper_position, gripper_quaternion, gripper_euler, cropped_color_image, cropped_depth_image, grasp_depth, grasp_width, grasp_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 14:05:49.975647 140365690476288 deprecation.py:323] From <ipython-input-3-621945151005>:9: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 3)\n",
      "(?, 4)\n",
      "(?, 50, 50, 3)\n",
      "(?, 50, 50, 1)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "training_dataset = tf.data.TFRecordDataset('./grasp_data/t_shape/training.tfrecord')\n",
    "training_dataset = training_dataset.map(_parse_function)\n",
    "\n",
    "training_dataset = training_dataset.repeat()\n",
    "training_dataset = training_dataset.batch(1)\n",
    "training_dataset = training_dataset.shuffle(buffer_size=1)\n",
    "\n",
    "training_iterator = training_dataset.make_initializable_iterator()\n",
    "training_gripper_position, training_gripper_quaternion, training_gripper_euler, training_cropped_color_image, training_cropped_depth_image, training_grasp_depth, training_grasp_width, training_grasp_quality = training_iterator.get_next()\n",
    "\n",
    "print(training_gripper_position.shape)\n",
    "print(training_gripper_quaternion.shape)\n",
    "print(training_cropped_color_image.shape)\n",
    "print(training_cropped_depth_image.shape)\n",
    "print(training_grasp_quality.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00941823  0.00907065 -0.928466  ]]\n",
      "[[0.96286094 0.11226846 0.08435628 0.23060487]]\n",
      "[[[[0.5294118  0.29803923 0.07843138]\n",
      "   [0.5137255  0.28627452 0.07450981]\n",
      "   [0.5137255  0.28627452 0.07450981]\n",
      "   ...\n",
      "   [0.5372549  0.32156864 0.10196079]\n",
      "   [0.5372549  0.32156864 0.10196079]\n",
      "   [0.5372549  0.32156864 0.10196079]]\n",
      "\n",
      "  [[0.5294118  0.29803923 0.07843138]\n",
      "   [0.5137255  0.28627452 0.07450981]\n",
      "   [0.5137255  0.28627452 0.07450981]\n",
      "   ...\n",
      "   [0.5372549  0.32156864 0.10196079]\n",
      "   [0.5372549  0.32156864 0.10196079]\n",
      "   [0.7176471  0.16078432 0.05098039]]\n",
      "\n",
      "  [[0.54509807 0.3137255  0.09019608]\n",
      "   [0.5411765  0.30980393 0.09019608]\n",
      "   [0.5411765  0.30980393 0.09019608]\n",
      "   ...\n",
      "   [0.69803923 0.13725491 0.03921569]\n",
      "   [0.9019608  0.         0.        ]\n",
      "   [0.9019608  0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.64705884 0.44705883 0.24705882]\n",
      "   [0.67058825 0.46666667 0.2627451 ]\n",
      "   [0.67058825 0.46666667 0.2627451 ]\n",
      "   ...\n",
      "   [0.5803922  0.37254903 0.16078432]\n",
      "   [0.5803922  0.37254903 0.16078432]\n",
      "   [0.5803922  0.37254903 0.16078432]]\n",
      "\n",
      "  [[0.64705884 0.44705883 0.24705882]\n",
      "   [0.64705884 0.44705883 0.23921569]\n",
      "   [0.64705884 0.44705883 0.23921569]\n",
      "   ...\n",
      "   [0.60784316 0.4        0.1882353 ]\n",
      "   [0.60784316 0.4        0.1882353 ]\n",
      "   [0.60784316 0.4        0.1882353 ]]\n",
      "\n",
      "  [[0.64705884 0.4509804  0.24705882]\n",
      "   [0.64705884 0.4509804  0.23921569]\n",
      "   [0.64705884 0.4509804  0.23921569]\n",
      "   ...\n",
      "   [0.60784316 0.4        0.1882353 ]\n",
      "   [0.60784316 0.4        0.1882353 ]\n",
      "   [0.60784316 0.4        0.1882353 ]]]]\n",
      "[[[[ 0.07339364]\n",
      "   [ 0.07352763]\n",
      "   [ 0.07366103]\n",
      "   ...\n",
      "   [ 0.07971495]\n",
      "   [ 0.0798499 ]\n",
      "   [ 0.07998496]]\n",
      "\n",
      "  [[ 0.07319075]\n",
      "   [ 0.07332474]\n",
      "   [ 0.07345718]\n",
      "   ...\n",
      "   [ 0.07950872]\n",
      "   [ 0.07964438]\n",
      "   [-0.03370941]]\n",
      "\n",
      "  [[ 0.07298714]\n",
      "   [ 0.07312113]\n",
      "   [ 0.07325429]\n",
      "   ...\n",
      "   [-0.03272742]\n",
      "   [-0.03361106]\n",
      "   [-0.03449237]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.06392777]\n",
      "   [ 0.06405932]\n",
      "   [ 0.06419086]\n",
      "   ...\n",
      "   [ 0.0701291 ]\n",
      "   [ 0.07026225]\n",
      "   [ 0.07039547]]\n",
      "\n",
      "  [[ 0.06372797]\n",
      "   [ 0.0638594 ]\n",
      "   [ 0.06399089]\n",
      "   ...\n",
      "   [ 0.06992674]\n",
      "   [ 0.0700599 ]\n",
      "   [ 0.07019299]]\n",
      "\n",
      "  [[ 0.06352895]\n",
      "   [ 0.06366038]\n",
      "   [ 0.06379181]\n",
      "   ...\n",
      "   [ 0.06972528]\n",
      "   [ 0.06985831]\n",
      "   [ 0.06999141]]]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(training_iterator.initializer)\n",
    "\n",
    "print(sess.run(training_gripper_position))\n",
    "print(sess.run(training_gripper_quaternion))\n",
    "print(sess.run(training_cropped_color_image))\n",
    "print(sess.run(training_cropped_depth_image))\n",
    "print(sess.run(training_grasp_quality))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
