{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyglet\n",
    "pyglet.options['shadow_window'] = False\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from tf import transformations\n",
    "\n",
    "from pyrender import IntrinsicsCamera,\\\n",
    "                     DirectionalLight, SpotLight, PointLight,\\\n",
    "                     MetallicRoughnessMaterial,\\\n",
    "                     Primitive, Mesh, Node, Scene,\\\n",
    "                     Viewer, OffscreenRenderer\n",
    "\n",
    "from grasp_proposal_from_mesh import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_matrix(euler, translation):\n",
    "    pose_matrix = transformations.euler_matrix(euler[0], euler[1], euler[2])\n",
    "    pose_matrix[:3,3] = translation\n",
    "    return np.asarray(pose_matrix)\n",
    "\n",
    "def rotate_vector(euler, vector):\n",
    "    return np.matmul(transformations.euler_matrix(euler[0], euler[1], euler[2])[:3,:3],vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.0\n",
      "2400\n",
      "0.06208333333333333\n"
     ]
    }
   ],
   "source": [
    "file_name = \"/home/dof6/.gazebo/models/t_shape/meshes/t_shape.dae\"\n",
    "\n",
    "object_trimesh, _ = load_dae_to_trimesh(file_name)\n",
    "gripper_contact_list, gripper_quaternion_list, gripper_width_list, quality_list, extra_information_list = sample_antipodal_grasp(object_trimesh,number_of_contact=100,number_of_quaternion=8)\n",
    "print(np.sum(quality_list))\n",
    "print(len(quality_list))\n",
    "print(np.sum(quality_list)/len(quality_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(object_trimesh):\n",
    "    object_face_colors = np.asarray([[1.,0.,0.]])*np.ones(object_trimesh.faces.shape)\n",
    "    object_trimesh.visual.face_colors = object_face_colors\n",
    "    object_mesh = Mesh.from_trimesh(object_trimesh, smooth=False)\n",
    "    object_pose = pose_matrix(np.pi*(2.*np.random.uniform(size=[3,])-1.),[0.,0.,0.1])\n",
    "    \n",
    "    point_l = PointLight(color=np.ones(3), intensity=10.0)\n",
    "\n",
    "    cam_width = 640\n",
    "    cam_height = 480\n",
    "    cam_fx = 521.179233\n",
    "    cam_fy = 493.033034\n",
    "    cam_cx = cam_width/2.\n",
    "    cam_cy = cam_height/2.\n",
    "    cam = IntrinsicsCamera(cam_fx,cam_fy,cam_cx,cam_cy)\n",
    "\n",
    "    cam_euler = np.pi/12.*(2.*np.random.uniform(size=[3,])-1.)\n",
    "    cam_euler[2] = 0.\n",
    "    cam_pose = pose_matrix(cam_euler,rotate_vector(cam_euler,[0.,0.,1.0]))\n",
    "\n",
    "    wood_trimesh = trimesh.load('./background_models/wood.obj')\n",
    "    wood_trimesh.vertices = wood_trimesh.vertices*10.\n",
    "    wood_mesh = Mesh.from_trimesh(wood_trimesh)\n",
    "\n",
    "    scene = Scene(ambient_light=np.array([0.02, 0.02, 0.02, 10.0]))\n",
    "\n",
    "    object_node = scene.add(object_mesh, pose=object_pose)\n",
    "    point_l_node = scene.add(point_l, pose=cam_pose)\n",
    "    wood_node = scene.add(wood_mesh)\n",
    "    cam_node = scene.add(cam, pose=cam_pose)\n",
    "\n",
    "    r = OffscreenRenderer(viewport_width=cam_width, viewport_height=cam_height,)\n",
    "    color, depth = r.render(scene)\n",
    "    r.delete()\n",
    "    return color, depth, cam_pose, cam, object_pose\n",
    "# color, depth, cam_pose, cam, object_pose = generate_image(object_trimesh)\n",
    "# cam_fx,cam_fy,cam_cx,cam_cy = cam.fx, cam.fy, cam.cx, cam.cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(color)\n",
    "# plt.figure()\n",
    "# plt.imshow(depth)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825\n",
      "0.1504424778761062\n",
      "2920\n",
      "0.1780821917808219\n",
      "2880\n",
      "0.16666666666666666\n",
      "2910\n",
      "0.17525773195876287\n",
      "2845\n",
      "0.15641476274165203\n",
      "2880\n",
      "0.16666666666666666\n",
      "3090\n",
      "0.22330097087378642\n",
      "2795\n",
      "0.1413237924865832\n",
      "2840\n",
      "0.15492957746478872\n",
      "2880\n",
      "0.16666666666666666\n",
      "2980\n",
      "0.19463087248322147\n",
      "2925\n",
      "0.1794871794871795\n",
      "2895\n",
      "0.17098445595854922\n",
      "2925\n",
      "0.1794871794871795\n",
      "2900\n",
      "0.1724137931034483\n",
      "3030\n",
      "0.2079207920792079\n",
      "2990\n",
      "0.19732441471571907\n",
      "2905\n",
      "0.1738382099827883\n",
      "3040\n",
      "0.21052631578947367\n",
      "2880\n",
      "0.16666666666666666\n",
      "2890\n",
      "0.1695501730103806\n",
      "2850\n",
      "0.15789473684210525\n",
      "2930\n",
      "0.18088737201365188\n",
      "2815\n",
      "0.14742451154529307\n",
      "3020\n",
      "0.2052980132450331\n",
      "2885\n",
      "0.1681109185441941\n",
      "3000\n",
      "0.2\n",
      "3045\n",
      "0.21182266009852216\n",
      "2830\n",
      "0.1519434628975265\n",
      "3075\n",
      "0.21951219512195122\n",
      "2925\n",
      "0.1794871794871795\n",
      "3075\n",
      "0.21951219512195122\n",
      "3090\n",
      "0.22330097087378642\n",
      "2915\n",
      "0.17667238421955403\n",
      "2885\n",
      "0.1681109185441941\n",
      "2920\n",
      "0.1780821917808219\n",
      "3035\n",
      "0.20922570016474465\n",
      "2895\n",
      "0.17098445595854922\n",
      "2965\n",
      "0.1905564924114671\n",
      "2885\n",
      "0.1681109185441941\n",
      "2940\n",
      "0.1836734693877551\n",
      "3025\n",
      "0.2066115702479339\n",
      "2850\n",
      "0.15789473684210525\n",
      "2825\n",
      "0.1504424778761062\n",
      "3100\n",
      "0.22580645161290322\n",
      "2975\n",
      "0.19327731092436976\n",
      "2965\n",
      "0.1905564924114671\n",
      "2925\n",
      "0.1794871794871795\n",
      "3000\n",
      "0.2\n",
      "2910\n",
      "0.17525773195876287\n"
     ]
    }
   ],
   "source": [
    "number_of_object_pose = 50\n",
    "visualize = False\n",
    "cropping_size = 25\n",
    "\n",
    "for i in range(number_of_object_pose):\n",
    "    color, depth, cam_pose, cam, object_pose = generate_image(object_trimesh)\n",
    "    cam_fx,cam_fy,cam_cx,cam_cy = cam.fx, cam.fy, cam.cx, cam.cy\n",
    "    cam_inv = np.linalg.inv(cam_pose)\n",
    "\n",
    "    grasp_quality_list = []\n",
    "    grasp_width_list = []\n",
    "    grasp_depth_list = []\n",
    "    eef_position_list = []\n",
    "    eef_position_p_list = []\n",
    "    eef_euler_list = []\n",
    "    eef_quaternion_list = []\n",
    "    all_eef_points_list = []\n",
    "    all_eef_points_p_list = []\n",
    "    cropped_color_image_list = []\n",
    "    cropped_depth_image_list = []\n",
    "\n",
    "    for gripper_contact, gripper_quaternion, gripper_width, quality, extra_information in zip(gripper_contact_list, gripper_quaternion_list, gripper_width_list, quality_list, extra_information_list):\n",
    "        eef_position = object_pose[:3,3] + np.matmul(object_pose[:3,:3], gripper_contact)\n",
    "        eef_quaternion = tf.transformations.quaternion_multiply(transformations.quaternion_from_matrix(object_pose), gripper_quaternion)\n",
    "        all_eef_points = object_pose[:3,[3]] + np.matmul(object_pose[:3,:3], extra_information['all_gripper_points'].T)\n",
    "\n",
    "        eef_position_cam_frame = cam_inv[:3,3] + np.matmul(cam_inv[:3,:3],eef_position)\n",
    "        eef_quaternion_cam_frame = tf.transformations.quaternion_multiply(transformations.quaternion_from_matrix(cam_inv), eef_quaternion)\n",
    "        gripper_depth = -eef_position_cam_frame[2]\n",
    "        eef_euler_cam_frame = tf.transformations.euler_from_quaternion(eef_quaternion_cam_frame)\n",
    "        approaching_direction = rotate_vector(eef_quaternion_cam_frame,[0.,0.,1.])\n",
    "\n",
    "        if approaching_direction[2] > np.cos(np.pi/6.):\n",
    "            quality = 0.\n",
    "\n",
    "        eef_px = -cam_fx*eef_position_cam_frame[0]/eef_position_cam_frame[2] + cam_cx\n",
    "        eef_py = cam_fy*eef_position_cam_frame[1]/eef_position_cam_frame[2] + cam_cy\n",
    "\n",
    "        center_x = int(eef_px)\n",
    "        center_y = int(eef_py)\n",
    "\n",
    "        cropped_color_image = color[center_y-cropping_size:center_y+cropping_size,center_x-cropping_size:center_x+cropping_size,:]\n",
    "        cropped_depth_image = depth[center_y-cropping_size:center_y+cropping_size,center_x-cropping_size:center_x+cropping_size,np.newaxis] - gripper_depth\n",
    "\n",
    "        all_eef_points_cam_frame = cam_inv[:3,[3]] +  np.matmul(cam_inv[:3,:3],all_eef_points)\n",
    "        all_eef_points_px = -cam_fx*all_eef_points_cam_frame[0,:]/all_eef_points_cam_frame[2,:] + cam_cx\n",
    "        all_eef_points_py = cam_fy*all_eef_points_cam_frame[1,:]/all_eef_points_cam_frame[2,:] + cam_cy\n",
    "        all_eef_points_p = np.concatenate([[all_eef_points_px],[all_eef_points_py]],axis=0).T\n",
    "        eef_p = np.concatenate([[eef_px],[eef_py]],axis=0).T\n",
    "\n",
    "        grasp_quality_list.append(quality)\n",
    "        eef_position_list.append(eef_position_cam_frame)\n",
    "        eef_position_p_list.append(eef_p)\n",
    "        eef_quaternion_list.append(eef_quaternion_cam_frame)\n",
    "        all_eef_points_list.append(all_eef_points_cam_frame)\n",
    "        all_eef_points_p_list.append(all_eef_points_p)\n",
    "        eef_euler_list.append(eef_euler_cam_frame)\n",
    "        cropped_color_image_list.append(cropped_color_image.astype(np.float32)/255.)\n",
    "        cropped_depth_image_list.append(cropped_depth_image)\n",
    "        grasp_width_list.append(gripper_width)\n",
    "        grasp_depth_list.append(gripper_depth)\n",
    "        \n",
    "        if quality==1.0:\n",
    "            for _ in range(4):\n",
    "                grasp_quality_list.append(quality)\n",
    "                eef_position_list.append(eef_position_cam_frame)\n",
    "                eef_position_p_list.append(eef_p)\n",
    "                eef_euler_list.append(eef_euler_cam_frame)\n",
    "                eef_quaternion_list.append(eef_quaternion_cam_frame)\n",
    "                all_eef_points_list.append(all_eef_points_cam_frame)\n",
    "                all_eef_points_p_list.append(all_eef_points_p)\n",
    "                cropped_color_image_list.append(cropped_color_image.astype(np.float32)/255.)\n",
    "                cropped_depth_image_list.append(cropped_depth_image)\n",
    "                grasp_width_list.append(gripper_width)\n",
    "                grasp_depth_list.append(gripper_depth)\n",
    "\n",
    "        if quality==1.0:\n",
    "            # TYPE 1:\n",
    "            random_direction = 2*np.pi*np.random.uniform()\n",
    "            center_x = int(eef_px + cropping_size*np.cos(random_direction))\n",
    "            center_y = int(eef_py + cropping_size*np.sin(random_direction))\n",
    "            cropped_color_image = color[center_y-cropping_size:center_y+cropping_size,center_x-cropping_size:center_x+cropping_size,:]\n",
    "            cropped_depth_image = depth[center_y-cropping_size:center_y+cropping_size,center_x-cropping_size:center_x+cropping_size,np.newaxis] - gripper_depth + 0.05*(2.*np.random.uniform() - 1.)\n",
    "\n",
    "            grasp_quality_list.append(0.0)\n",
    "            eef_position_list.append(eef_position_cam_frame)\n",
    "            eef_position_p_list.append(eef_p)\n",
    "            eef_euler_list.append(eef_euler_cam_frame)\n",
    "            eef_quaternion_list.append(eef_quaternion_cam_frame)\n",
    "            all_eef_points_list.append(all_eef_points_cam_frame)\n",
    "            all_eef_points_p_list.append(all_eef_points_p)\n",
    "            cropped_color_image_list.append(cropped_color_image.astype(np.float32)/255.)\n",
    "            cropped_depth_image_list.append(cropped_depth_image)\n",
    "            grasp_width_list.append(gripper_width)\n",
    "            grasp_depth_list.append(gripper_depth)\n",
    "            \n",
    "        if visualize:\n",
    "            plt.figure()\n",
    "            plt.imshow(cropped_color_image)\n",
    "            plt.figure()\n",
    "            plt.imshow(cropped_depth_image[:,:,0])\n",
    "            plt.plot(cropping_size,cropping_size,'ko')\n",
    "            plt.plot(all_eef_points_px - center_x + cropping_size,all_eef_points_py - center_y + cropping_size,'k-')\n",
    "            plt.show()\n",
    "            break\n",
    "            \n",
    "    if visualize:\n",
    "        break\n",
    "        \n",
    "    print(len(grasp_quality_list))\n",
    "    print(np.sum(grasp_quality_list)/len(grasp_quality_list))\n",
    "    np.save(\"./grasp_data/t_shape/\"+str(i)+\"-\"+str(number_of_object_pose)+\".npy\",{\n",
    "        'grasp_quality_list' : grasp_quality_list,\n",
    "        'eef_position_list' : eef_position_list,\n",
    "        'eef_position_p_list' : eef_position_p_list,\n",
    "        'eef_quaternion_list' : eef_quaternion_list,\n",
    "        'eef_euler_list' : eef_euler_list,\n",
    "        'all_eef_points_list' : all_eef_points_list,\n",
    "        'all_eef_points_p_list' : all_eef_points_p_list,\n",
    "        'cropped_color_image_list' : cropped_color_image_list,\n",
    "        'cropped_depth_image_list' : cropped_depth_image_list,\n",
    "        'grasp_width_list' : grasp_width_list,\n",
    "        'grasp_depth_list' : grasp_depth_list,\n",
    "        'cam_fx' : cam_fx,\n",
    "        'cam_fy' : cam_fy,\n",
    "        'cam_cx' : cam_cx,\n",
    "        'cam_cy' : cam_cy,\n",
    "        'cam_pose' : cam_pose\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
